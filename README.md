--- **Iris** é um projeto de robô humanoide que se comunica com o usuário por meio de um **chat de voz**, oferecendo conversas amigáveis ou respondendo a perguntas diversas. Durante a interação, Iris simula **feições faciais pré-definidas** para criar uma experiência mais expressiva. Além disso, Iris é capaz de **reconhecer objetos e expressões faciais** por meio de uma **câmera integrada**. ### **Tecnologias e Bibliotecas Usadas** 1. **Reconhecimento de Voz e Texto** - **Whisper (OpenAI)**: Para transcrição de áudio em texto de forma precisa. Essa biblioteca será responsável por capturar as entradas de voz do usuário. - **OpenAI GPT**: Para gerar respostas inteligentes e contextuais baseadas no texto capturado. - **gTTS ou pyttsx3**: Para sintetizar as respostas de texto em áudio e devolvê-las ao usuário. 2. **Visão Computacional** - **OpenCV**: Para processar os frames capturados pela câmera e realizar operações básicas de visão computacional. - **YOLO (You Only Look Once)**: Para detecção em tempo real de objetos no ambiente do usuário. - **MediaPipe**: Para rastreamento de expressões faciais e reconhecimento de emoções, como felicidade, surpresa ou raiva. 3. **Expressividade e Feições Faciais** - **OpenCV e Animações Digitais**: Caso o robô possua uma interface digital (ex.: um display no rosto), expressões faciais animadas podem ser renderizadas dinamicamente. - **Motores Servo e Raspberry Pi GPIO**: Para criar movimentos físicos no rosto do robô, como sobrancelhas móveis ou um sorriso mecânico. ### **Funcionalidades de Iris** 1. **Chat de Voz Amigável** - O áudio do usuário é capturado e processado pelo **Whisper**, que o converte em texto. - O texto é enviado à **API OpenAI GPT**, que responde com base no contexto da conversa. - A resposta é convertida para áudio e reproduzida para o usuário. 2. **Reconhecimento de Objetos** - Usando **YOLO**, Iris pode identificar objetos em tempo real por meio da câmera integrada. - As informações são verbalizadas, como: _"Eu vejo uma cadeira na sua frente."_ 3. **Reconhecimento de Expressões Faciais** - O **MediaPipe** identifica as expressões do usuário (sorriso, surpresa, etc.), permitindo que Iris adapte suas respostas ou feições correspondentes. 4. **Feições Faciais Pré-Definidas** - **Digital**: A face do robô pode ser exibida em um monitor integrado, alternando entre animações de diferentes emoções. - **Física**: Motores servo, controlados por **Python**, permitem que o robô exiba expressões reais, como levantar as sobrancelhas ou movimentar a boca. ### **Fluxo do Sistema** 1. **Entrada de Áudio**: O microfone captura o áudio do usuário e o processa com **Whisper** para transcrição. 2. **Processamento de Linguagem**: O texto transcrito é enviado ao **GPT**, que gera uma resposta apropriada. 3. **Resposta por Voz**: A resposta é convertida para áudio com **gTTS** ou **pyttsx3** e reproduzida. 4. **Análise do Ambiente**: A câmera captura frames para identificar objetos com **YOLO** ou expressões faciais com **MediaPipe**. 5. **Expressão e Feedback**: Iris ajusta sua face (digital ou física) para reagir à interação do usuário. --- ### **Estrutura do Projeto** - **Hardware**: - Câmera USB ou Raspberry Pi Camera Module. - Raspberry Pi ou computador com GPIO para controle de motores servo. - Microfone e alto-falante para entrada e saída de áudio. - **Software**: - **Python** para toda a lógica de programação. - **Whisper** e **OpenAI GPT** para processamento de linguagem. - **YOLO**, **MediaPipe** e **OpenCV** para visão computacional. - **Flask** ou **FastAPI** (opcional) para hospedar serviços de interação. Esse formato utiliza os recursos mais avançados dessas bibliotecas e é adaptável para várias plataformas.
